{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "HAP_simulation.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6c2bbf3-dd50-4942-b6ac-2c6c2c851013"
      },
      "source": [
        "# imports\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "def simulate_haplotypes(read_length, std_read_length, \n",
        "                        coverage, ref_length=int(2.5e2),\n",
        "                        false_variance=0.0,\n",
        "                        switch_error_rate=0.0,\n",
        "                        miscall_error_rate=0.0,\n",
        "                        missing_error_rate=0.0):\n",
        "    \"\"\"\n",
        "    param read_length: Avg read length.\n",
        "    param std_read_length How the read lengths is distributed around the read_length, which is avg read length. \n",
        "    param coverage: Number of avegage reads per genome for the reference over snps\n",
        "    param reference_length: THe length of the reference haptype \n",
        "    param false_variance:\n",
        "    param switch_error_rate: \n",
        "    param missing_error_rate:\n",
        "    param miscall_error_rate:\n",
        "    \"\"\"\n",
        "    read_length_bak = read_length\n",
        "\n",
        "    read_length = int(read_length)\n",
        "    mate_length = read_length\n",
        "    num_reads_for_read_coverage = int(ref_length * coverage / read_length)\n",
        "    \n",
        "    # Create a reference Haplotype:\n",
        "    p = 0.5\n",
        "    reference_hap1 = np.random.choice(a=[False, True], size=(ref_length), p=[p, 1-p])\n",
        "    reference_hap2 = np.logical_not(reference_hap1)\n",
        "    haps = [reference_hap1, reference_hap2]\n",
        "    \n",
        "    # False Variance \n",
        "    # Lets assume there is false variance for the sequence with some probablity.\n",
        "\n",
        "\n",
        "    # The sequence haptype we get has some false variance, i.e. some of the \n",
        "    # snps are homologous. \n",
        "\n",
        "\n",
        "    # sequencing error is a common error, we should simulate the sequencing \n",
        "    # error too. \n",
        "  \n",
        "    \n",
        "    # Pick the length of the read length from a normal distribution centered at\n",
        "    # read_lenth and variance as std_read_length\n",
        "    read_lengths = np.array(\n",
        "        np.random.normal(loc = read_length, scale = std_read_length, \n",
        "                         size = num_reads_for_read_coverage), \n",
        "                         dtype=\"int32\")\n",
        "    \n",
        "    # Uniformy choose start points from the reference haplotype, h1 or h2. \n",
        "    reads_start_idx = np.random.randint(low = 0, high=ref_length, size=num_reads_for_read_coverage)\n",
        "    \n",
        "    reads_st_en = []\n",
        "    #reads = np.array([reads_start_idx, reads_start_idx + read_length])\n",
        "    for st_idx, read_len in zip(reads_start_idx, read_lengths):\n",
        "      if st_idx + read_len > ref_length - 1:\n",
        "        en = ref_length - 1\n",
        "      else:\n",
        "        en = st_idx + read_len\n",
        "      reads_st_en.append( (st_idx, en ) )\n",
        "    #reads_st_en = np.array(reads_st_en)\n",
        "    #import pdb;pdb.set_trace()\n",
        "    # We have the st and en of the reads, choose either h1 or h2 from H and \n",
        "    # sample it. Sample from S1 if True else sample from S2.\n",
        "    h1_or_h2 = np.array(\n",
        "        np.less(np.random.uniform(0, 1, num_reads_for_read_coverage), 0.5), \n",
        "        dtype=\"int\")\n",
        "     \n",
        "    hap_samples = [haps[v][st:en] for v, (st, en) in zip(h1_or_h2, reads_st_en)]\n",
        "    #pdb.set_trace()\n",
        "    # Simulate switch errors:\n",
        "\n",
        "    sw_hap_samples = []\n",
        "    fragfilecontent = []\n",
        "    # Get the switch error for each of the sample.\n",
        "    # Get the missing error for each of the sample \n",
        "    qual = '~' if miscall_error_rate < 1e-9 else str(int(-10*math.log10(miscall_error_rate)))\n",
        "    \n",
        "    \n",
        "    for sample, x in zip(hap_samples, reads_st_en ):\n",
        "      #import pdb;pdb.set_trace()  \n",
        "      \n",
        "      assert len(sample) == abs(x[1] - x[0])  \n",
        "      switch_error = np.less( np.random.uniform(0, 1, size=len(sample)) , switch_error_rate)\n",
        "      miscall_error = np.less(np.random.uniform(0, 1, size=len(sample)) , miscall_error_rate)      \n",
        "      missing_error = np.less(np.random.uniform(0, 1, size=len(sample)) , missing_error_rate)\n",
        "    \n",
        "      # Simulate switch errors\n",
        "      switch_idxs = list(np.nonzero(switch_error))\n",
        "      is_switched = False\n",
        "      new_sample = []\n",
        "      for sa, sw in zip(sample, switch_error):\n",
        "        if sw:\n",
        "          is_switched = not is_switched\n",
        "          if is_switched:\n",
        "            new_sample.append(not sa)\n",
        "          else:\n",
        "            new_sample.append(sa) \n",
        "        else:\n",
        "            new_sample.append(sa) \n",
        "            \n",
        "      updated_sample = new_sample\n",
        "      assert len(updated_sample) == abs(x[1] - x[0])\n",
        "\n",
        "      # Simulate miscall errors      \n",
        "      new_sample = []\n",
        "      for sa, miscall in zip(updated_sample, miscall_error):\n",
        "        if miscall:\n",
        "          new_sample.append(not sa)\n",
        "        else:\n",
        "          new_sample.append(sa)\n",
        "      \n",
        "      updated_sample = new_sample \n",
        "      assert len(updated_sample) == abs(x[1] - x[0])\n",
        "\n",
        "      # Simulate missing errors  \n",
        "      new_sample = []\n",
        "\n",
        "      for sa, missing in zip(updated_sample, missing_error):\n",
        "        if missing:\n",
        "          new_sample.append(-1)\n",
        "        else:\n",
        "          new_sample.append(sa)\n",
        "      assert len(new_sample) == abs(x[1] - x[0])\n",
        "      # INdex of the variant, reference hap value, misscall rate\n",
        "        \n",
        "      fragfilecontent.append((x, new_sample, qual))      \n",
        "      sw_hap_samples.append(new_sample)\n",
        "        \n",
        "    #import pdb;pdb.set_trace()\n",
        "    # Update the hap samples with the new samples\n",
        "    hap_samples = [list(np.array(s, dtype=\"int32\")) for s in sw_hap_samples]\n",
        "    \n",
        "\n",
        "    assert len(hap_samples) == num_reads_for_read_coverage\n",
        "    return hap_samples, reads_st_en, fragfilecontent, haps"
      ],
      "id": "b6c2bbf3-dd50-4942-b6ac-2c6c2c851013",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfdc4128-7b87-4312-9985-43ba1697a9ba"
      },
      "source": [
        "ref_length = int(30)\n",
        "hap_samples, st_en, frag_file_contents, ref_H = simulate_haplotypes(read_length=5, std_read_length=2, \n",
        "                                                                coverage=3, ref_length=ref_length,\n",
        "                                                                false_variance=0.1,\n",
        "                                                                switch_error_rate=0.1,\n",
        "                                                                miscall_error_rate=0.1,\n",
        "                                                                missing_error_rate=0.02)"
      ],
      "id": "bfdc4128-7b87-4312-9985-43ba1697a9ba",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6c679ce-eec2-4df5-aaa0-24e957877c1c",
        "outputId": "92171242-d17e-487d-8260-bcceb95757dd"
      },
      "source": [
        "miscall_error_rate = 0.02\n",
        "q = '~' if miscall_error_rate < 5.011872336272714e-10 else chr(int(33-10*math.log10(miscall_error_rate)))\n",
        "for x, sa in zip(st_en, hap_samples):\n",
        "    print(x, sa, q)"
      ],
      "id": "d6c679ce-eec2-4df5-aaa0-24e957877c1c",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18, 21) [1, 0, 1] 1\n",
            "(5, 8) [1, 1, 1] 1\n",
            "(11, 17) [1, 1, 1, 1, 1, 0] 1\n",
            "(7, 11) [0, 0, 1, 0] 1\n",
            "(4, 9) [0, 0, 1, 0, 0] 1\n",
            "(15, 18) [1, 0, 0] 1\n",
            "(13, 17) [0, 1, 1, 0] 1\n",
            "(1, 5) [0, 1, 0, 1] 1\n",
            "(10, 13) [1, 1, 0] 1\n",
            "(23, 28) [0, 0, 1, 1, 0] 1\n",
            "(14, 20) [1, 1, 0, 1, 1, 1] 1\n",
            "(22, 27) [0, 0, 0, 1, 1] 1\n",
            "(5, 11) [1, 1, 0, 0, 0, 1] 1\n",
            "(9, 12) [1, 0, 0] 1\n",
            "(20, 24) [1, 0, 1, 0] 1\n",
            "(0, 5) [0, 0, 0, 0, 0] 1\n",
            "(12, 15) [1, 1, 1] 1\n",
            "(29, 29) [] 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22c65c6d-8121-463d-ba30-757820ff34e8",
        "outputId": "4959fa69-1aa2-453e-97c7-19cc97bbaced"
      },
      "source": [
        "def generate_matrix_for_visualization(hap_samples, reference_length, st_en):\n",
        "    matrix = [[\"-\" for _ in range(reference_length)] for _ in range(len(hap_samples))]\n",
        "    for idx, ( (s,e),  sa ) in enumerate(zip(st_en, hap_samples)):\n",
        "        for i, v in zip(range(s, e), sa):\n",
        "            if v != -1:\n",
        "                matrix[idx][i] = v\n",
        "                \n",
        "    for m in matrix:\n",
        "        print(\" \".join(str(v) for v in m) )\n",
        "        \n",
        "        \n",
        "        \n",
        "generate_matrix_for_visualization(hap_samples, 30, st_en) "
      ],
      "id": "22c65c6d-8121-463d-ba30-757820ff34e8",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- - - - - - - - - - - - - - - - - - 1 0 1 - - - - - - - - -\n",
            "- - - - - 1 1 1 - - - - - - - - - - - - - - - - - - - - - -\n",
            "- - - - - - - - - - - 1 1 1 1 1 0 - - - - - - - - - - - - -\n",
            "- - - - - - - 0 0 1 0 - - - - - - - - - - - - - - - - - - -\n",
            "- - - - 0 0 1 0 0 - - - - - - - - - - - - - - - - - - - - -\n",
            "- - - - - - - - - - - - - - - 1 0 0 - - - - - - - - - - - -\n",
            "- - - - - - - - - - - - - 0 1 1 0 - - - - - - - - - - - - -\n",
            "- 0 1 0 1 - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "- - - - - - - - - - 1 1 0 - - - - - - - - - - - - - - - - -\n",
            "- - - - - - - - - - - - - - - - - - - - - - - 0 0 1 1 0 - -\n",
            "- - - - - - - - - - - - - - 1 1 0 1 1 1 - - - - - - - - - -\n",
            "- - - - - - - - - - - - - - - - - - - - - - 0 0 0 1 1 - - -\n",
            "- - - - - 1 1 0 0 0 1 - - - - - - - - - - - - - - - - - - -\n",
            "- - - - - - - - - 1 0 0 - - - - - - - - - - - - - - - - - -\n",
            "- - - - - - - - - - - - - - - - - - - - 1 0 1 0 - - - - - -\n",
            "0 0 0 0 0 - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "- - - - - - - - - - - - 1 1 1 - - - - - - - - - - - - - - -\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed19296a-8e65-4197-b10f-ce21cf87dfa4"
      },
      "source": [
        "#print(frag_file_contents)"
      ],
      "id": "ed19296a-8e65-4197-b10f-ce21cf87dfa4",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc764548-9f54-46ed-b2f0-e333c8a240d6",
        "outputId": "9e22bac3-3350-4d7e-fd2b-89c97a4c14f1"
      },
      "source": [
        "def print_fragment(frag_data):\n",
        "    # build line content\n",
        "    # INdex of the variant, reference hap value, misscall rate\n",
        "    frags = []\n",
        "    for (st,en), alleles, qual in frag_data:\n",
        "        fs1 = [(idx, str(int(val)), qual) for idx, val in zip(range(st, en), alleles)]\n",
        "        frags.append(fs1)\n",
        "           \n",
        "    for idx, fs in enumerate(frags):\n",
        "        name = \"FRAG{}\".format(idx+1)\n",
        "        fragstr = ''\n",
        "        num_pairs = 0\n",
        "        prev_snp_ix = -2\n",
        "        qual = ' '\n",
        "        for snp_ix, allele, q_char in fs:\n",
        "\n",
        "            diff = snp_ix - prev_snp_ix\n",
        "\n",
        "            if diff == 1:\n",
        "                fragstr += allele\n",
        "            else:\n",
        "                num_pairs += 1\n",
        "                fragstr += ' {} {}'.format(snp_ix+1, allele)\n",
        "\n",
        "            prev_snp_ix = snp_ix\n",
        "            qual += q_char\n",
        "\n",
        "        fragstr += qual\n",
        "\n",
        "        prefix = '{} {}'.format(num_pairs,name)\n",
        "        fragstr = prefix + fragstr\n",
        "\n",
        "        # print line to file\n",
        "        print(fragstr)\n",
        "\n",
        "\n",
        "    # with open(\"sample.fragments\", \"r\") as fd:\n",
        "    #   lines = fd.readlines()\n",
        "    # print(\"\".join(lines))  \n",
        "print_fragment(frag_file_contents)"
      ],
      "id": "dc764548-9f54-46ed-b2f0-e333c8a240d6",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 FRAG1 19 101 101010\n",
            "1 FRAG2 6 111 101010\n",
            "1 FRAG3 12 111110 101010101010\n",
            "1 FRAG4 8 0010 10101010\n",
            "1 FRAG5 5 00100 1010101010\n",
            "1 FRAG6 16 100 101010\n",
            "1 FRAG7 14 0110 10101010\n",
            "1 FRAG8 2 0101 10101010\n",
            "1 FRAG9 11 110 101010\n",
            "1 FRAG10 24 00110 1010101010\n",
            "1 FRAG11 15 110111 101010101010\n",
            "1 FRAG12 23 00011 1010101010\n",
            "1 FRAG13 6 110001 101010101010\n",
            "1 FRAG14 10 100 101010\n",
            "1 FRAG15 21 1010 10101010\n",
            "1 FRAG16 1 00000 1010101010\n",
            "1 FRAG17 13 111 101010\n",
            "0 FRAG18 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2295ce0f-a634-4998-ac34-096ef7ec2e9b",
        "outputId": "eea39f8c-c30d-4d6b-ddb9-cbbc3e73d820"
      },
      "source": [
        "def print_vcf_file_format(st_en, H):\n",
        "    ref_name = \"ref_name\"\n",
        "    ref_length = len(H[0])\n",
        "    header ='''##fileformat=VCFv4.1\n",
        "##contig=<ID={},length={}>\n",
        "##INFO=<ID=mt,Number=1,Type=String,Description=\"Variant Type: SUBSTITUTE/INSERT/DELETE\">\n",
        "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n",
        "##FORMAT=<ID=FP,Number=1,Type=Integer,Description=\"Read Depth\">\n",
        "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tSIM_INDIVIDUAL'''.format(ref_name,ref_length)\n",
        "    \n",
        "    print(header)\n",
        "    bases = ['A','T','G','C']\n",
        "    for snp in range(ref_length):\n",
        "        ref_ix = random.randrange(0,4)\n",
        "        alt_ix = ref_ix\n",
        "        while(alt_ix == ref_ix):\n",
        "            alt_ix = random.randrange(0,4)\n",
        "\n",
        "        genotype_field = '{}|{}'.format(int(H[0][snp]),int(H[1][snp]))\n",
        "        ID = '.'\n",
        "        ref_snp = bases[ref_ix]\n",
        "        alt_snp = bases[alt_ix]\n",
        "        qual = 100\n",
        "        fltr = 'PASS'\n",
        "        info = 'mt=SUBSTITUTE'\n",
        "        print('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\tGT\\t{}'.format(ref_name, snp, ID, ref_snp, alt_snp, qual, fltr, info, genotype_field))\n",
        "    # with open(\"sample.vcf\", \"r\") as fd:\n",
        "    #   lines = fd.readlines()\n",
        "    # print(len(lines))  \n",
        "    # print(\"\".join(lines))   \n",
        "print_vcf_file_format(st_en,  ref_H)           "
      ],
      "id": "2295ce0f-a634-4998-ac34-096ef7ec2e9b",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##fileformat=VCFv4.1\n",
            "##contig=<ID=ref_name,length=30>\n",
            "##INFO=<ID=mt,Number=1,Type=String,Description=\"Variant Type: SUBSTITUTE/INSERT/DELETE\">\n",
            "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n",
            "##FORMAT=<ID=FP,Number=1,Type=Integer,Description=\"Read Depth\">\n",
            "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tSIM_INDIVIDUAL\n",
            "ref_name\t0\t.\tG\tA\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t1\t.\tT\tC\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t2\t.\tA\tC\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t3\t.\tC\tG\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t4\t.\tG\tA\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t5\t.\tC\tT\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t6\t.\tC\tA\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t7\t.\tT\tC\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t8\t.\tG\tC\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t9\t.\tA\tT\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t10\t.\tA\tG\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t11\t.\tA\tC\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t12\t.\tG\tC\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t13\t.\tT\tG\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t14\t.\tG\tC\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t15\t.\tA\tT\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t16\t.\tT\tG\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t17\t.\tA\tT\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t18\t.\tA\tG\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t19\t.\tT\tA\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t20\t.\tA\tT\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t21\t.\tT\tA\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t22\t.\tA\tT\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t23\t.\tG\tA\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t24\t.\tT\tG\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t25\t.\tT\tC\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t26\t.\tC\tT\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n",
            "ref_name\t27\t.\tT\tG\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t28\t.\tT\tG\t100\tPASS\tmt=SUBSTITUTE\tGT\t0|1\n",
            "ref_name\t29\t.\tT\tA\t100\tPASS\tmt=SUBSTITUTE\tGT\t1|0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBCFBeahA9KB"
      },
      "source": [
        ""
      ],
      "id": "LBCFBeahA9KB",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcxYpmTZWEXK"
      },
      "source": [
        ""
      ],
      "id": "vcxYpmTZWEXK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGNisT-_WJaJ"
      },
      "source": [
        ""
      ],
      "id": "xGNisT-_WJaJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lb_vsWDwELC"
      },
      "source": [
        "Notes: We want to incorporate sequencing errors in the fragments f1 and f2 and get the confidence whether f1 and f2 belong from the same haplotype. Use some probablity to guess whether f1 and f2 belong to the same haplotype. \n",
        "\n",
        "Now once we have a probablity that f1 and f2 belong to the same haplotype, we want to check whether a variant is a false variant.\n",
        "\n",
        "What is a false variant ? \n",
        "False variant is a location where the alleles are homozygous but the sequencing \n",
        "assumes / wrongly interprets the site as heterozygous. \n",
        "Hitherto, in the fragments (for these false variants) there would be a substantial amount of fragments where this site has the same allele say 0 for \n",
        "both fragments coming from both H1 and H2 (as it is not really a variant hence same allele is observed irrespective sample from H1 or H2 (0, 0) in the given position).\n",
        "\n",
        "However for sequencing errors the errors are randomly distributed among different variants for different fragments.\n",
        "\n",
        "Problem statement, we want to identify these false variant locations for the \n",
        "sequenced data, as these become difficult to handle while performing Hap assembly.\n",
        "\n",
        "When we have a good coverage for a particular false variant, \n",
        "if we have fragment f1 which belongs to H1 and a fragment f2 which belongs to H2, the value for the variants are complement of each other except for the false variant. \n",
        "\n",
        "Now we dont know whether the fragment belongs to either H1 or H2, "
      ],
      "id": "6lb_vsWDwELC"
    }
  ]
}